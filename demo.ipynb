{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3d6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModel, BertTokenizerFast, ElectraTokenizer, ElectraModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import random\n",
    "from transformers import AdamW\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cdfb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeting seed for PyTorch, Python i Numpy\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c8a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentiment(value):\n",
    "    if value.startswith('+'):\n",
    "        return 1\n",
    "    elif value.startswith('-'):\n",
    "        return 0\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6952ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\Faks\\Master\\Web mining\\Projekat\\podaci_1.csv\", encoding=\"cp1252\")\n",
    "df['label'] = df['label'].apply(transform_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b707f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"D:\\Faks\\Master\\Web mining\\Projekat\\podaci_2.csv\")\n",
    "df_test = df_test[['1', 'comment']]\n",
    "df_test = df_test.rename(columns = {'1' : 'label', 'comment' : 'text'})\n",
    "df_test['label'] = df_test['label'].apply(transform_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8beb2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "# split test dataset into test_text and test_labels\n",
    "test_text = df_test['text']\n",
    "test_labels = df_test['label']                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4023382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-multilingual-cased')\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ea969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76fa11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the parameters of first 8 layers\n",
    "for name, param in bert.named_parameters():\n",
    "    if 'encoder.layer' in name:\n",
    "        layer_index = int(name.split('.')[2])\n",
    "        if layer_index < 3:\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "339eec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 50,\n",
    "    padding = 'longest',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 50,\n",
    "    padding = 'longest',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 50,\n",
    "    padding = 'longest',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "## convert lists to tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.to_list())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c684578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      #self.fc2 = nn.Linear(512,3)\n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b62ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train(model, train_dataloader, device, cross_entropy, optimizer):\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "594ff057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate(model, val_dataloader, device, cross_entropy):\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    # if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "      # elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      # print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "860baba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(batch_size, device, learning_rate, epochs, optimizer_name, model_name):\n",
    "    \n",
    "    # prepare train and validation dataset\n",
    "    train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
    "    \n",
    "    # specify device\n",
    "    device = torch.device(device)\n",
    "    model = BERT_Arch(bert)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # spcecify learning_rate and optimizer\n",
    "    if optimizer_name == 'adamw':\n",
    "        optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    # copmute and set class_weights\n",
    "    class_weights = compute_class_weight(class_weight = \"balanced\", classes = np.unique(train_labels), y = train_labels)\n",
    "    weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "    weights = weights.to(device)\n",
    "    # cross_entropy  = nn.NLLLoss(weight=weights)\n",
    "    cross_entropy  = nn.NLLLoss()\n",
    "\n",
    "    # set initial loss to infinite\n",
    "    best_valid_loss = float('inf')\n",
    "    # empty lists to store training and validation loss and accuracy of each epoch\n",
    "    train_losses=[]\n",
    "    valid_losses=[]\n",
    "    train_accuracies = []  # lista za čuvanje tačnosti tokom treninga\n",
    "    valid_accuracies = []  # lista za čuvanje tačnosti tokom evaluacije\n",
    "\n",
    "    #for each epoch\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "        \n",
    "        #train and evaluate model\n",
    "        train_loss, _ = train(model, train_dataloader, device, cross_entropy, optimizer)\n",
    "        valid_loss, _ = evaluate(model, val_dataloader, device, cross_entropy)\n",
    "\n",
    "        #save the best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "\n",
    "        # append training and validation loss and accuracy\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "    \n",
    "\n",
    "        print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "        print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e1218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(path, batch_size, device):\n",
    "\n",
    "    # pass the pre-trained BERT to our define architecture\n",
    "    model_test = BERT_Arch(bert)\n",
    "    #load weights of best model\n",
    "    model_test.load_state_dict(torch.load(path))\n",
    "    model_test.to(device)\n",
    "\n",
    "    num_rows = test_seq.shape[0]\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, num_rows, batch_size):\n",
    "            start_idx = i\n",
    "            end_idx = min(i + batch_size, num_rows)\n",
    "            batch_seq = test_seq[start_idx:end_idx].to(device)\n",
    "            batch_mask = test_mask[start_idx:end_idx].to(device)\n",
    "\n",
    "            batch_preds = model_test(batch_seq.to(device), batch_mask.to(device))\n",
    "            batch_preds = batch_preds.detach().cpu().numpy()\n",
    "            preds.append(batch_preds)\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    pred_labels = np.argmax(preds, axis=1)\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(test_y, pred_labels))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    confusion_mat = confusion_matrix(test_y, pred_labels)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_mat)\n",
    "\n",
    "    # Compute ROC curve and AUC score\n",
    "    fpr, tpr, thresholds = roc_curve(test_y, preds[:, 1])\n",
    "    auc_score = roc_auc_score(test_y, preds[:, 1])\n",
    "    print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1203cd6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TensorDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8060\\4082535443.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"adamw\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"bert_4_epochs.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8060\\4000580521.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(batch_size, device, learning_rate, epochs, optimizer_name, model_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# prepare train and validation dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_sampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_sampler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TensorDataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(32, \"cpu\", 2e-5, 4, \"adamw\", \"bert_4_epochs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b73914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73       191\n",
      "           1       0.84      0.74      0.79       273\n",
      "\n",
      "    accuracy                           0.76       464\n",
      "   macro avg       0.76      0.77      0.76       464\n",
      "weighted avg       0.77      0.76      0.76       464\n",
      "\n",
      "Confusion Matrix:\n",
      "[[151  40]\n",
      " [ 70 203]]\n",
      "AUC Score: 0.8371018161594077\n"
     ]
    }
   ],
   "source": [
    "test(\"bert_4_epochs.pt\", 10, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a7e0d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikola\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.674\n",
      "Validation Loss: 0.670\n",
      "\n",
      " Epoch 2 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.665\n",
      "Validation Loss: 0.668\n",
      "\n",
      " Epoch 3 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.661\n",
      "Validation Loss: 0.668\n",
      "\n",
      " Epoch 4 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.664\n",
      "Validation Loss: 0.666\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(32, \"cuda\", 2e-5, 4, \"adamw\", \"bert_4_epochs_freezed.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96ac79e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       191\n",
      "           1       0.59      1.00      0.74       273\n",
      "\n",
      "    accuracy                           0.59       464\n",
      "   macro avg       0.29      0.50      0.37       464\n",
      "weighted avg       0.35      0.59      0.44       464\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0 191]\n",
      " [  0 273]]\n",
      "AUC Score: 0.5500834244289741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikola\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Nikola\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Nikola\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test(\"bert_4_epochs_freezed.pt\", 10, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c10d191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 2\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.678\n",
      "Validation Loss: 0.670\n",
      "\n",
      " Epoch 2 / 2\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.667\n",
      "Validation Loss: 0.669\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(32, \"cpu\", 2e-5, 2, \"adamw\", \"bert_3_epochs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40859db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikola\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 3\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.641\n",
      "Validation Loss: 0.607\n",
      "\n",
      " Epoch 2 / 3\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.534\n",
      "Validation Loss: 0.544\n",
      "\n",
      " Epoch 3 / 3\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.396\n",
      "Validation Loss: 0.537\n",
      "Execution time: 52.49 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_and_evaluate(32, \"cpu\", 2e-5, 3, \"adamw\", \"bert_3_epochs.pt\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time) / 60 \n",
    "print(f\"Execution time: {execution_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76af4156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73       191\n",
      "           1       0.83      0.74      0.78       273\n",
      "\n",
      "    accuracy                           0.76       464\n",
      "   macro avg       0.76      0.77      0.76       464\n",
      "weighted avg       0.77      0.76      0.76       464\n",
      "\n",
      "Confusion Matrix:\n",
      "[[151  40]\n",
      " [ 71 202]]\n",
      "AUC Score: 0.8342826458009702\n"
     ]
    }
   ],
   "source": [
    "test(\"bert_3_epochs.pt\", 10, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7128bcfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikola\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.671\n",
      "Validation Loss: 0.669\n",
      "\n",
      " Epoch 2 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.665\n",
      "Validation Loss: 0.669\n",
      "\n",
      " Epoch 3 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.665\n",
      "Validation Loss: 0.667\n",
      "\n",
      " Epoch 4 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.663\n",
      "Validation Loss: 0.663\n",
      "\n",
      " Epoch 5 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.661\n",
      "Validation Loss: 0.664\n",
      "\n",
      " Epoch 6 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.661\n",
      "Validation Loss: 0.659\n",
      "\n",
      " Epoch 7 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.657\n",
      "Validation Loss: 0.659\n",
      "\n",
      " Epoch 8 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.656\n",
      "Validation Loss: 0.656\n",
      "\n",
      " Epoch 9 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.656\n",
      "Validation Loss: 0.655\n",
      "\n",
      " Epoch 10 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.654\n",
      "Validation Loss: 0.653\n",
      "\n",
      " Epoch 11 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.653\n",
      "Validation Loss: 0.653\n",
      "\n",
      " Epoch 12 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.650\n",
      "Validation Loss: 0.652\n",
      "\n",
      " Epoch 13 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.654\n",
      "Validation Loss: 0.651\n",
      "\n",
      " Epoch 14 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.648\n",
      "Validation Loss: 0.649\n",
      "\n",
      " Epoch 15 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.647\n",
      "Validation Loss: 0.650\n",
      "\n",
      " Epoch 16 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.646\n",
      "Validation Loss: 0.653\n",
      "\n",
      " Epoch 17 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.646\n",
      "Validation Loss: 0.649\n",
      "\n",
      " Epoch 18 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.648\n",
      "Validation Loss: 0.645\n",
      "\n",
      " Epoch 19 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.642\n",
      "Validation Loss: 0.646\n",
      "\n",
      " Epoch 20 / 20\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.642\n",
      "Validation Loss: 0.645\n",
      "Execution time: 9.17 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_and_evaluate(32, \"cuda\", 2e-5, 20, \"adamw\", \"bert_20_epochs_freezed.pt\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time) / 60 \n",
    "print(f\"Execution time: {execution_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6de9e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.05      0.10       191\n",
      "           1       0.60      0.99      0.75       273\n",
      "\n",
      "    accuracy                           0.61       464\n",
      "   macro avg       0.72      0.52      0.42       464\n",
      "weighted avg       0.70      0.61      0.48       464\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 10 181]\n",
      " [  2 271]]\n",
      "AUC Score: 0.6766008860249697\n"
     ]
    }
   ],
   "source": [
    "test(\"bert_20_epochs_freezed.pt\", 10, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5395ad71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikola\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.660\n",
      "Validation Loss: 0.649\n",
      "\n",
      " Epoch 2 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.622\n",
      "Validation Loss: 0.621\n",
      "\n",
      " Epoch 3 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.585\n",
      "Validation Loss: 0.616\n",
      "\n",
      " Epoch 4 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.548\n",
      "Validation Loss: 0.609\n",
      "Execution time: 2.89 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_and_evaluate(32, \"cuda\", 2e-5, 4, \"adamw\", \"bert_4_epochs_10_freezed.pt\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time) / 60 \n",
    "print(f\"Execution time: {execution_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb550089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.55      0.58       191\n",
      "           1       0.71      0.77      0.74       273\n",
      "\n",
      "    accuracy                           0.68       464\n",
      "   macro avg       0.67      0.66      0.66       464\n",
      "weighted avg       0.67      0.68      0.68       464\n",
      "\n",
      "Confusion Matrix:\n",
      "[[105  86]\n",
      " [ 63 210]]\n",
      "AUC Score: 0.7519705425464587\n"
     ]
    }
   ],
   "source": [
    "test(\"bert_4_epochs_10_freezed.pt\", 10, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6b567c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikola\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.660\n",
      "Validation Loss: 0.649\n",
      "\n",
      " Epoch 2 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.622\n",
      "Validation Loss: 0.621\n",
      "\n",
      " Epoch 3 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.585\n",
      "Validation Loss: 0.616\n",
      "\n",
      " Epoch 4 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.548\n",
      "Validation Loss: 0.609\n",
      "\n",
      " Epoch 5 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.521\n",
      "Validation Loss: 0.610\n",
      "\n",
      " Epoch 6 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.492\n",
      "Validation Loss: 0.607\n",
      "\n",
      " Epoch 7 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.439\n",
      "Validation Loss: 0.699\n",
      "\n",
      " Epoch 8 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.424\n",
      "Validation Loss: 0.734\n",
      "\n",
      " Epoch 9 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.379\n",
      "Validation Loss: 0.683\n",
      "\n",
      " Epoch 10 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.354\n",
      "Validation Loss: 0.735\n",
      "\n",
      " Epoch 11 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.351\n",
      "Validation Loss: 0.691\n",
      "\n",
      " Epoch 12 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.295\n",
      "Validation Loss: 0.754\n",
      "\n",
      " Epoch 13 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.259\n",
      "Validation Loss: 0.848\n",
      "\n",
      " Epoch 14 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.235\n",
      "Validation Loss: 0.813\n",
      "\n",
      " Epoch 15 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.227\n",
      "Validation Loss: 0.869\n",
      "\n",
      " Epoch 16 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.191\n",
      "Validation Loss: 0.888\n",
      "\n",
      " Epoch 17 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.154\n",
      "Validation Loss: 0.965\n",
      "\n",
      " Epoch 18 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.156\n",
      "Validation Loss: 1.008\n",
      "\n",
      " Epoch 19 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.136\n",
      "Validation Loss: 1.026\n",
      "\n",
      " Epoch 20 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.122\n",
      "Validation Loss: 1.144\n",
      "\n",
      " Epoch 21 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.105\n",
      "Validation Loss: 1.307\n",
      "\n",
      " Epoch 22 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.091\n",
      "Validation Loss: 1.261\n",
      "\n",
      " Epoch 23 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.106\n",
      "Validation Loss: 1.321\n",
      "\n",
      " Epoch 24 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.080\n",
      "Validation Loss: 1.348\n",
      "\n",
      " Epoch 25 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.088\n",
      "Validation Loss: 1.329\n",
      "\n",
      " Epoch 26 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.069\n",
      "Validation Loss: 1.394\n",
      "\n",
      " Epoch 27 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.072\n",
      "Validation Loss: 1.537\n",
      "\n",
      " Epoch 28 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.079\n",
      "Validation Loss: 1.583\n",
      "\n",
      " Epoch 29 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.077\n",
      "Validation Loss: 1.654\n",
      "\n",
      " Epoch 30 / 30\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.051\n",
      "Validation Loss: 1.641\n",
      "Execution time: 20.43 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_and_evaluate(32, \"cuda\", 2e-5, 30, \"adamw\", \"bert_30_epochs_10_freezed.pt\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time) / 60 \n",
    "print(f\"Execution time: {execution_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee6e3418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65       191\n",
      "           1       0.78      0.63      0.69       273\n",
      "\n",
      "    accuracy                           0.67       464\n",
      "   macro avg       0.68      0.68      0.67       464\n",
      "weighted avg       0.70      0.67      0.68       464\n",
      "\n",
      "Confusion Matrix:\n",
      "[[142  49]\n",
      " [102 171]]\n",
      "AUC Score: 0.7736033599907945\n"
     ]
    }
   ],
   "source": [
    "test(\"bert_30_epochs_10_freezed.pt\", 10, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7188b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikola\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 6\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.660\n",
      "Validation Loss: 0.649\n",
      "\n",
      " Epoch 2 / 6\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.622\n",
      "Validation Loss: 0.621\n",
      "\n",
      " Epoch 3 / 6\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.585\n",
      "Validation Loss: 0.616\n",
      "\n",
      " Epoch 4 / 6\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.548\n",
      "Validation Loss: 0.609\n",
      "\n",
      " Epoch 5 / 6\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.521\n",
      "Validation Loss: 0.610\n",
      "\n",
      " Epoch 6 / 6\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.492\n",
      "Validation Loss: 0.607\n",
      "Execution time: 4.24 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_and_evaluate(32, \"cuda\", 2e-5, 6, \"adamw\", \"bert_6_epochs_10_freezed.pt\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time) / 60 \n",
    "print(f\"Execution time: {execution_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84e4c9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65       191\n",
      "           1       0.78      0.63      0.70       273\n",
      "\n",
      "    accuracy                           0.68       464\n",
      "   macro avg       0.68      0.69      0.68       464\n",
      "weighted avg       0.70      0.68      0.68       464\n",
      "\n",
      "Confusion Matrix:\n",
      "[[142  49]\n",
      " [101 172]]\n",
      "AUC Score: 0.7736033599907945\n"
     ]
    }
   ],
   "source": [
    "test(\"bert_6_epochs_10_freezed.pt\", 10, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14c751df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikola\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.649\n",
      "Validation Loss: 0.629\n",
      "\n",
      " Epoch 2 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.593\n",
      "Validation Loss: 0.604\n",
      "\n",
      " Epoch 3 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.499\n",
      "Validation Loss: 0.599\n",
      "\n",
      " Epoch 4 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.420\n",
      "Validation Loss: 0.646\n",
      "Execution time: 64.63 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_and_evaluate(32, \"cpu\", 2e-5, 4, \"adamw\", \"bert_4_epochs_6_freezed.pt\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time) / 60 \n",
    "print(f\"Execution time: {execution_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed2caf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.72      0.67       191\n",
      "           1       0.78      0.71      0.74       273\n",
      "\n",
      "    accuracy                           0.71       464\n",
      "   macro avg       0.71      0.71      0.71       464\n",
      "weighted avg       0.72      0.71      0.72       464\n",
      "\n",
      "Confusion Matrix:\n",
      "[[137  54]\n",
      " [ 79 194]]\n",
      "AUC Score: 0.7807375870203096\n"
     ]
    }
   ],
   "source": [
    "test(\"bert_4_epochs_6_freezed.pt\", 10, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0323634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikola\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.647\n",
      "Validation Loss: 0.626\n",
      "\n",
      " Epoch 2 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.563\n",
      "Validation Loss: 0.590\n",
      "\n",
      " Epoch 3 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.443\n",
      "Validation Loss: 0.588\n",
      "\n",
      " Epoch 4 / 4\n",
      "  Batch    50  of     77.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.345\n",
      "Validation Loss: 0.614\n",
      "Execution time: 66.81 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_and_evaluate(32, \"cpu\", 2e-5, 4, \"adamw\", \"bert_4_epochs_3_freezed.pt\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time) / 60 \n",
    "print(f\"Execution time: {execution_time:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65ce014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.84      0.74       191\n",
      "           1       0.86      0.69      0.77       273\n",
      "\n",
      "    accuracy                           0.75       464\n",
      "   macro avg       0.76      0.77      0.75       464\n",
      "weighted avg       0.78      0.75      0.76       464\n",
      "\n",
      "Confusion Matrix:\n",
      "[[161  30]\n",
      " [ 84 189]]\n",
      "AUC Score: 0.8307538883455114\n"
     ]
    }
   ],
   "source": [
    "test(\"bert_4_epochs_3_freezed.pt\", 10, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cda1a7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 4\n",
      "  Batch    50  of    306.\n",
      "  Batch   100  of    306.\n",
      "  Batch   150  of    306.\n",
      "  Batch   200  of    306.\n",
      "  Batch   250  of    306.\n",
      "  Batch   300  of    306.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.399\n",
      "Validation Loss: 1.008\n",
      "\n",
      " Epoch 2 / 4\n",
      "  Batch    50  of    306.\n",
      "  Batch   100  of    306.\n",
      "  Batch   150  of    306.\n",
      "  Batch   200  of    306.\n",
      "  Batch   250  of    306.\n",
      "  Batch   300  of    306.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.306\n",
      "Validation Loss: 1.112\n",
      "\n",
      " Epoch 3 / 4\n",
      "  Batch    50  of    306.\n",
      "  Batch   100  of    306.\n",
      "  Batch   150  of    306.\n",
      "  Batch   200  of    306.\n",
      "  Batch   250  of    306.\n",
      "  Batch   300  of    306.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.269\n",
      "Validation Loss: 1.249\n",
      "\n",
      " Epoch 4 / 4\n",
      "  Batch    50  of    306.\n",
      "  Batch   100  of    306.\n",
      "  Batch   150  of    306.\n",
      "  Batch   200  of    306.\n",
      "  Batch   250  of    306.\n",
      "  Batch   300  of    306.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.164\n",
      "Validation Loss: 1.295\n",
      "\n",
      " Epoch 1 / 4\n",
      "  Batch    50  of    153.\n",
      "  Batch   100  of    153.\n",
      "  Batch   150  of    153.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.147\n",
      "Validation Loss: 1.256\n",
      "\n",
      " Epoch 2 / 4\n",
      "  Batch    50  of    153.\n",
      "  Batch   100  of    153.\n",
      "  Batch   150  of    153.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.088\n",
      "Validation Loss: 1.370\n",
      "\n",
      " Epoch 3 / 4\n",
      "  Batch    50  of    153.\n",
      "  Batch   100  of    153.\n",
      "  Batch   150  of    153.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.070\n",
      "Validation Loss: 1.538\n",
      "\n",
      " Epoch 4 / 4\n",
      "  Batch    50  of    153.\n",
      "  Batch   100  of    153.\n",
      "  Batch   150  of    153.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.089\n",
      "Validation Loss: 1.503\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [8, 16]\n",
    "for batch_size in batch_sizes:\n",
    "    model_name = \"bert_4_epochs_unfreezed_\" + str(batch_size) +\".pt\"\n",
    "    train_and_evaluate(batch_size, \"cpu\", 2e-5, 4, \"adamw\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3328538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.71       191\n",
      "           1       0.79      0.80      0.80       273\n",
      "\n",
      "    accuracy                           0.76       464\n",
      "   macro avg       0.75      0.75      0.75       464\n",
      "weighted avg       0.76      0.76      0.76       464\n",
      "\n",
      "Confusion Matrix:\n",
      "[[134  57]\n",
      " [ 54 219]]\n",
      "AUC Score: 0.8347812745718505\n"
     ]
    }
   ],
   "source": [
    "test(\"bert_4_epochs_unfreezed_8.pt\", 10, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1468136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72       191\n",
      "           1       0.83      0.74      0.78       273\n",
      "\n",
      "    accuracy                           0.76       464\n",
      "   macro avg       0.75      0.76      0.75       464\n",
      "weighted avg       0.76      0.76      0.76       464\n",
      "\n",
      "Confusion Matrix:\n",
      "[[148  43]\n",
      " [ 70 203]]\n",
      "AUC Score: 0.7900197533705386\n"
     ]
    }
   ],
   "source": [
    "test(\"bert_4_epochs_unfreezed_16.pt\", 10, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977869b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
